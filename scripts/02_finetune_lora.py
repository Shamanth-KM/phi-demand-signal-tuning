# -*- coding: utf-8 -*-
"""02_finetune_lora.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Shamanth-KM/phi-demand-intent-lora/blob/main/notebooks/02_finetune_lora.ipynb

# 02 - Fine-Tuning Preparation: Loading and Tokenizing Sales Notes Dataset

In this notebook, we load the previously generated synthetic dataset and prepare it for fine-tuning.  
We tokenize the sales notes using Phi-1.5 tokenizer and prepare a Hugging Face Dataset.
"""

# Installing the necessary libraries
!pip install -q --upgrade transformers datasets peft accelerate

# Importing required libraries
import pandas as pd
from datasets import Dataset
from transformers import AutoTokenizer

"""## Upload Sales Notes CSV

We upload the previously saved 'sales_notes_2000.csv' file containing the synthetic dataset.

"""

# Let's upload the csv file needed
from google.colab import files
uploaded_file = files.upload()

# Get the file name
file_name = list(uploaded_file.keys())[0]

# Read the file to a pandas DataFrame
sales_notes = pd.read_csv(file_name)

# Check the first few rows
sales_notes.head(10)

"""## Convert to Hugging Face Dataset
We convert the DataFrame into a Hugging Face Dataset for easier tokenization and batching.

## Load Phi-1.5 Tokenizer
We load the Phi-1.5 tokenizer and set up necessary padding.

## Tokenize the Dataset
We tokenize the sales notes to create model-readable input tensors.
"""

# Convert it into Hugging Face Dataset format
sales_hf = Dataset.from_pandas(sales_notes)
print(sales_hf)

# Loading the Phi-1.5 tokenizer
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1_5", trust_remote_code=True)

# Phi-1.5's tokenizer does not have a pad_token defined by default.
'''
Decoder models (like GPT, Phi) usually don't need padding when generating text â€” but for training a classification model,
we do need padding to batch inputs of different lengths.
'''
tokenizer.pad_token = tokenizer.eos_token

# Checking on tokenizer
print("Tokenizer loaded. Trying a sample encoding:")
print(tokenizer("There's order request for a old stock - Neural Ninjas"))

# Let's define a tokenization function
def tokenize_function(examples):
    return tokenizer(
        examples["sales_note"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

# Applying tokenization
sales_tokenized = sales_hf.map(tokenize_function, batched=True)

# Checking the structure after tokenization
print(sales_tokenized)

"""## Encode Labels

We map each demand category to a numeric label for classification.

## Label Mapping

Initially, the sales notes dataset contains human-readable labels (e.g., "Repeat Order", "Urgent Need").  
For training, we map these labels to numeric IDs using a label_to_id dictionary.  
Later during evaluation and inference, we reverse-map predictions back to human-readable labels using an id_to_label mapping.

This approach ensures that the model works with numerical labels internally while remaining interpretable externally.
"""

# Create label mapping
sales_labels = list(set(sales_notes["label"]))
sales_labels.sort()
label_to_id = {label: idx for idx, label in enumerate(sales_labels)}
id_to_label = {idx: label for label, idx in label_to_id.items()}

print("Label Mapping:", label_to_id)

# Apply label encoding
def encode_labels(example):
    return {"labels": label_to_id[example["label"]]}

sales_tokenized = sales_tokenized.map(encode_labels)

# Remove old label column
sales_tokenized = sales_tokenized.remove_columns(["label"])

print("Labels encoded successfully!")

"""## Train-Validation Split

We split the dataset into 60% training and 40% validation sets.
"""

# Split the dataset
sales_split = sales_tokenized.train_test_split(test_size=0.4, seed=42)

train_sales = sales_split["train"]
val_sales = sales_split["test"]

print(f"Dataset split done! Training size: {len(train_sales)}, Validation size: {len(val_sales)}")

"""# Summary
- Loaded the sales notes dataset.
- Tokenized the dataset using Phi-1.5 tokenizer.
- Encoded text labels into numeric form.
- Split the dataset into training and validation sets.

Next, we proceed to model loading, LoRA configuration, and fine-tuning.
"""